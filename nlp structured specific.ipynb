{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a2f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset: imdb\n",
      "Analyzing NLP dataset: imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report saved at reports/imdb_nlp_analysis_report.pdf\n",
      "Analyzing dataset: titanic\n",
      "Analyzing Structured dataset: titanic\n",
      "PDF report saved at reports/titanic_structured_analysis_report.pdf\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from fpdf import FPDF\n",
    "from scipy import stats\n",
    "from datasets import load_dataset\n",
    "import wordcloud  # For word cloud\n",
    "import re  # For text cleaning\n",
    "\n",
    "class ComprehensiveDatasetAnalyzer:\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "        \n",
    "        # Remove common stopwords\n",
    "        stopwords = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', \n",
    "                         'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', \n",
    "                         'into', 'over', 'after', 'is', 'are', 'was', 'were'])\n",
    "        \n",
    "        # Split and filter out stopwords\n",
    "        words = [word for word in text.split() if word not in stopwords]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def generate_word_cloud(self, dataset_name, data):\n",
    "        output_dir = f\"visualizations/nlp_{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Combine all text\n",
    "        all_text = ' '.join(data['text'].apply(self.clean_text))\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud_generator = wordcloud.WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white', \n",
    "            max_words=200, \n",
    "            colormap='viridis'\n",
    "        ).generate(all_text)\n",
    "\n",
    "        # Plot the word cloud\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud - {dataset_name.upper()} Dataset')\n",
    "        plt.tight_layout(pad=0)\n",
    "        \n",
    "        # Save the word cloud\n",
    "        plt.savefig(f\"{output_dir}/word_cloud.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_nlp_data(self, dataset_name, data):\n",
    "        print(f\"Analyzing NLP dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        # Generate word cloud\n",
    "        self.generate_word_cloud(dataset_name, data)\n",
    "\n",
    "        # Enhanced text statistics\n",
    "        data['text_length'] = data['text'].apply(len)\n",
    "        data['word_count'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "        data['unique_word_count'] = data['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "        # Detailed text length analysis\n",
    "        results['text_length_stats'] = {\n",
    "            'basic_stats': data['text_length'].describe(),\n",
    "            'skewness': stats.skew(data['text_length']),\n",
    "            'kurtosis': stats.kurtosis(data['text_length'])\n",
    "        }\n",
    "\n",
    "        # Word count analysis\n",
    "        results['word_count_stats'] = {\n",
    "            'basic_stats': data['word_count'].describe(),\n",
    "            'vocabulary_diversity': (data['unique_word_count'] / data['word_count']).mean()\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        # Label distribution\n",
    "        if 'label' in data.columns:\n",
    "            results['label_distribution'] = data['label'].value_counts(normalize=True)\n",
    "\n",
    "        # Text complexity metrics\n",
    "        def text_complexity(text):\n",
    "            words = str(text).split()\n",
    "            complex_words = [word for word in words if len(word) > 6]\n",
    "            return len(complex_words) / len(words) if words else 0\n",
    "\n",
    "        data['text_complexity'] = data['text'].apply(text_complexity)\n",
    "        results['text_complexity'] = {\n",
    "            'mean_complexity': data['text_complexity'].mean(),\n",
    "            'complexity_distribution': data['text_complexity'].describe()\n",
    "        }\n",
    "\n",
    "        # Generate visualizations\n",
    "        self._generate_nlp_visualizations(dataset_name, data, results)\n",
    "\n",
    "        # Export detailed PDF report\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"nlp\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _generate_nlp_visualizations(self, dataset_name, data, results):\n",
    "        output_dir = f\"visualizations/nlp_{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Text length distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data['text_length'], kde=True)\n",
    "        plt.title(f\"Text Length Distribution - {dataset_name}\")\n",
    "        plt.xlabel(\"Text Length\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/text_length_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # Label distribution (if exists)\n",
    "        if 'label' in data.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            data['label'].value_counts().plot(kind='bar')\n",
    "            plt.title(f\"Label Distribution - {dataset_name}\")\n",
    "            plt.xlabel(\"Label\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/label_distribution.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # Text complexity boxplot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=data['text_complexity'])\n",
    "        plt.title(f\"Text Complexity Distribution - {dataset_name}\")\n",
    "        plt.xlabel(\"Text Complexity (Proportion of Complex Words)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/text_complexity_boxplot.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_structured_data(self, dataset_name, data):\n",
    "        print(f\"Analyzing Structured dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        # Comprehensive data overview\n",
    "        results['dataset_overview'] = {\n",
    "            'total_rows': len(data),\n",
    "            'total_columns': len(data.columns),\n",
    "            'column_types': data.dtypes.to_dict()\n",
    "        }\n",
    "\n",
    "        # Detailed descriptive statistics for numeric columns\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        results['numeric_stats'] = data[numeric_cols].describe().to_dict()\n",
    "\n",
    "        # Advanced statistical analysis\n",
    "        results['statistical_tests'] = {}\n",
    "        for col in numeric_cols:\n",
    "            # Normality test\n",
    "            _, p_value = stats.normaltest(data[col].dropna())\n",
    "            results['statistical_tests'][col] = {\n",
    "                'normality_p_value': p_value,\n",
    "                'is_normally_distributed': p_value > 0.05\n",
    "            }\n",
    "\n",
    "        # Null value and duplicate analysis\n",
    "        results['data_quality'] = {\n",
    "            'null_values': data.isnull().sum().to_dict(),\n",
    "            'null_percentage': (data.isnull().sum() / len(data) * 100).to_dict(),\n",
    "            'duplicate_rows': data[data.duplicated()].shape[0]\n",
    "        }\n",
    "\n",
    "        # Categorical variable analysis\n",
    "        cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "        results['categorical_analysis'] = {\n",
    "            col: data[col].value_counts(normalize=True).to_dict() for col in cat_cols\n",
    "        }\n",
    "\n",
    "        # Correlation analysis\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        results['correlation_analysis'] = {\n",
    "            'correlation_matrix': numeric_data.corr().to_dict(),\n",
    "            'high_correlations': self._find_high_correlations(numeric_data)\n",
    "        }\n",
    "\n",
    "        # Outlier detection for numeric columns\n",
    "        results['outlier_analysis'] = {\n",
    "            col: self._detect_outliers(data[col]) for col in numeric_cols\n",
    "        }\n",
    "\n",
    "        # Generate visualizations\n",
    "        self._generate_structured_visualizations(dataset_name, data, results)\n",
    "\n",
    "        # Export detailed PDF report\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"structured\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _find_high_correlations(self, data, threshold=0.7):\n",
    "        corr_matrix = data.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        return [(col1, col2, corr_matrix.loc[col1, col2]) \n",
    "                for col1 in upper.columns \n",
    "                for col2 in upper.index \n",
    "                if upper.loc[col1, col2] > threshold]\n",
    "\n",
    "    def _detect_outliers(self, series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "        return {\n",
    "            'outliers': outliers.tolist(),\n",
    "            'outlier_percentage': len(outliers) / len(series) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        }\n",
    "\n",
    "    def _generate_structured_visualizations(self, dataset_name, data, results):\n",
    "        output_dir = f\"visualizations/structured_{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Correlation heatmap for numeric data\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title(f\"Correlation Heatmap - {dataset_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/correlation_heatmap.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Boxplot for numeric features\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        numeric_data.boxplot()\n",
    "        plt.title(f\"Boxplot of Numeric Features - {dataset_name}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/numeric_boxplot.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Categorical variable distribution\n",
    "        cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            data[col].value_counts().plot(kind='bar')\n",
    "            plt.title(f\"Distribution of {col} - {dataset_name}\")\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/{col}_distribution.png\")\n",
    "            plt.close()\n",
    "\n",
    "    def export_pdf_report(self, dataset_name, analysis_results, data_type=\"general\"):\n",
    "        \"\"\"Export a comprehensive analysis report as a PDF.\"\"\"\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=f\"{data_type.upper()} Analysis Report for {dataset_name}\", ln=True, align=\"C\")\n",
    "        pdf.ln(10)\n",
    "\n",
    "        def write_dict_to_pdf(d, level=0):\n",
    "            for key, value in d.items():\n",
    "                pdf.set_font(\"Arial\", style=\"B\", size=10 + level)\n",
    "                pdf.cell(200, 10, txt=\"  \" * level + str(key), ln=True)\n",
    "                pdf.set_font(\"Arial\", size=10 - level)\n",
    "                \n",
    "                if isinstance(value, (dict, np.ndarray)):\n",
    "                    write_dict_to_pdf(value, level + 1)\n",
    "                else:\n",
    "                    pdf.multi_cell(0, 10, txt=str(value))\n",
    "                pdf.ln(5)\n",
    "\n",
    "        write_dict_to_pdf(analysis_results)\n",
    "\n",
    "        output_path = f\"reports/{dataset_name}_{data_type}_analysis_report.pdf\"\n",
    "        os.makedirs(\"reports\", exist_ok=True)\n",
    "        pdf.output(output_path)\n",
    "        print(f\"PDF report saved at {output_path}\")\n",
    "\n",
    "    def perform_comprehensive_analysis(self):\n",
    "        comprehensive_results = {}\n",
    "\n",
    "        for dataset_name in self.datasets:\n",
    "            print(f\"Analyzing dataset: {dataset_name}\")\n",
    "\n",
    "            try:\n",
    "                if dataset_name == \"imdb\":  # NLP Example\n",
    "                    # Load full IMDB dataset from Hugging Face\n",
    "                    dataset = load_dataset(\"imdb\", split='train')\n",
    "                    \n",
    "                    # Convert to pandas DataFrame\n",
    "                    data = pd.DataFrame(dataset)\n",
    "                    \n",
    "                    # Perform analysis\n",
    "                    comprehensive_results[dataset_name] = self.analyze_nlp_data(dataset_name, data)\n",
    "\n",
    "                elif dataset_name == \"titanic\":  # Structured Example\n",
    "                    # Load Titanic dataset\n",
    "                    data = sns.load_dataset(\"titanic\")\n",
    "                    \n",
    "                    # Perform analysis\n",
    "                    comprehensive_results[dataset_name] = self.analyze_structured_data(dataset_name, data)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unknown dataset type for {dataset_name}, skipping.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {dataset_name}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        return comprehensive_results\n",
    "\n",
    "    def analyze_structured_data(self, dataset_name, data):\n",
    "        print(f\"Analyzing Structured dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        # Comprehensive data overview\n",
    "        results['dataset_overview'] = {\n",
    "            'total_rows': len(data),\n",
    "            'total_columns': len(data.columns),\n",
    "            'column_types': data.dtypes.to_dict()\n",
    "        }\n",
    "\n",
    "        # Null value analysis with location tracking\n",
    "        null_analysis = {}\n",
    "        for column in data.columns:\n",
    "            null_rows = data[data[column].isnull()]\n",
    "            if not null_rows.empty:\n",
    "                null_analysis[column] = {\n",
    "                    'total_null': len(null_rows),\n",
    "                    'null_percentage': (len(null_rows) / len(data)) * 100,\n",
    "                    'row_indices': null_rows.index.tolist()\n",
    "                }\n",
    "        results['null_value_analysis'] = null_analysis\n",
    "\n",
    "        # Duplicate rows analysis with location tracking\n",
    "        duplicate_rows = data[data.duplicated(keep=False)]\n",
    "        results['duplicate_rows_analysis'] = {\n",
    "            'total_duplicates': len(duplicate_rows),\n",
    "            'duplicate_percentage': (len(duplicate_rows) / len(data)) * 100,\n",
    "            'duplicate_row_indices': duplicate_rows.index.tolist(),\n",
    "            'duplicate_details': duplicate_rows.to_dict(orient='records') if not duplicate_rows.empty else {}\n",
    "        }\n",
    "\n",
    "        # Detailed descriptive statistics for numeric columns\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        results['numeric_stats'] = data[numeric_cols].describe().to_dict()\n",
    "\n",
    "        # Advanced statistical analysis\n",
    "        results['statistical_tests'] = {}\n",
    "        for col in numeric_cols:\n",
    "            # Normality test\n",
    "            _, p_value = stats.normaltest(data[col].dropna())\n",
    "            results['statistical_tests'][col] = {\n",
    "                'normality_p_value': p_value,\n",
    "                'is_normally_distributed': p_value > 0.05\n",
    "            }\n",
    "\n",
    "        # Outlier detection with location tracking\n",
    "        outlier_analysis = {}\n",
    "        for col in numeric_cols:\n",
    "            outliers_info = self._detect_outliers_with_location(data[col])\n",
    "            if outliers_info['outliers']:\n",
    "                outlier_analysis[col] = outliers_info\n",
    "        results['outlier_analysis'] = outlier_analysis\n",
    "\n",
    "        # Categorical variable analysis\n",
    "        cat_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "        results['categorical_analysis'] = {\n",
    "            col: data[col].value_counts(normalize=True).to_dict() for col in cat_cols\n",
    "        }\n",
    "\n",
    "        # Correlation analysis\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        results['correlation_analysis'] = {\n",
    "            'correlation_matrix': numeric_data.corr().to_dict(),\n",
    "            'high_correlations': self._find_high_correlations(numeric_data)\n",
    "        }\n",
    "\n",
    "        # Generate visualizations\n",
    "        self._generate_structured_visualizations(dataset_name, data, results)\n",
    "\n",
    "        # Export detailed PDF report\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"structured\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _detect_outliers_with_location(self, series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Detect outliers\n",
    "        outlier_mask = (series < lower_bound) | (series > upper_bound)\n",
    "        outliers = series[outlier_mask]\n",
    "        \n",
    "        return {\n",
    "            'outliers': outliers.tolist(),\n",
    "            'outlier_row_indices': outliers.index.tolist(),\n",
    "            'outlier_details': [\n",
    "                {\n",
    "                    'row_index': idx, \n",
    "                    'value': value, \n",
    "                    'type': 'lower_outlier' if value < lower_bound else 'upper_outlier'\n",
    "                } \n",
    "                for idx, value in outliers.items()\n",
    "            ],\n",
    "            'outlier_percentage': (len(outliers) / len(series)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        }\n",
    "\n",
    "\n",
    "    def export_pdf_report(self, dataset_name, analysis_results, data_type=\"general\"):\n",
    "\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=f\"{data_type.upper()} Analysis Report for {dataset_name}\", ln=True, align=\"C\")\n",
    "        pdf.ln(10)\n",
    "\n",
    "        def write_dict_to_pdf(d, level=0):\n",
    "            for key, value in d.items():\n",
    "                pdf.set_font(\"Arial\", style=\"B\", size=10 + level)\n",
    "                pdf.cell(200, 10, txt=\"  \" * level + str(key), ln=True)\n",
    "                pdf.set_font(\"Arial\", size=10 - level)\n",
    "                \n",
    "                if isinstance(value, (dict, list, np.ndarray)):\n",
    "                    # Handle nested structures with more detailed formatting\n",
    "                    if isinstance(value, dict):\n",
    "                        write_dict_to_pdf(value, level + 1)\n",
    "                    else:\n",
    "                        # Limit the number of items shown to prevent overwhelming the PDF\n",
    "                        limited_value = value[:50] if len(value) > 50 else value\n",
    "                        pdf.multi_cell(0, 10, txt=str(limited_value))\n",
    "                        if len(value) > 50:\n",
    "                            pdf.multi_cell(0, 10, txt=\"... (truncated)\")\n",
    "                else:\n",
    "                    pdf.multi_cell(0, 10, txt=str(value))\n",
    "                pdf.ln(5)\n",
    "\n",
    "        write_dict_to_pdf(analysis_results)\n",
    "\n",
    "        output_path = f\"reports/{dataset_name}_{data_type}_analysis_report.pdf\"\n",
    "        os.makedirs(\"reports\", exist_ok=True)\n",
    "        pdf.output(output_path)\n",
    "        print(f\"PDF report saved at {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze both IMDB and Titanic datasets\n",
    "    datasets_to_analyze = [\"imdb\", \"titanic\"]\n",
    "    analyzer = ComprehensiveDatasetAnalyzer(datasets_to_analyze)\n",
    "    results = analyzer.perform_comprehensive_analysis()\n",
    "    print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e924a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
