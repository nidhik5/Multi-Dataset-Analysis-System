{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f97f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 9.05k/9.05k [00:00<00:00, 4.73MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1.03M/1.03M [00:00<00:00, 1.04MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 127k/127k [00:00<00:00, 192kB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 129k/129k [00:00<00:00, 194kB/s]\n",
      "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████| 16000/16000 [00:00<00:00, 144164.81 examples/s]\n",
      "Generating validation split: 100%|██████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 285200.69 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 198490.56 examples/s]\n",
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Report generated: analysis_workspace/analysis_report_20241216_222655.md\n",
      "Analysis complete. Report generated: analysis_workspace/analysis_report_20241216_222655.md\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class DataAnalysisSystem:\n",
    "    def __init__(self):\n",
    "        # Create workspace directory\n",
    "        os.makedirs(\"analysis_workspace\", exist_ok=True)\n",
    "        os.makedirs(\"analysis_workspace/visualizations\", exist_ok=True)\n",
    "\n",
    "    def load_and_process_dataset(self, dataset_name: str, subset: str = None):\n",
    "        \"\"\"Load dataset from Hugging Face and perform initial processing\"\"\"\n",
    "        try:\n",
    "            print(f\"Attempting to load dataset: {dataset_name}\")\n",
    "            \n",
    "            # Try to load the dataset\n",
    "            dataset = load_dataset(dataset_name, subset) if subset else load_dataset(dataset_name)\n",
    "            \n",
    "            # Determine which split to use\n",
    "            if 'train' in dataset:\n",
    "                split = 'train'\n",
    "            elif len(dataset.keys()) > 0:\n",
    "                split = list(dataset.keys())[0]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable data split found\")\n",
    "            \n",
    "            return self._convert_to_pandas(dataset[split])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _convert_to_pandas(self, dataset):\n",
    "        \"\"\"Convert Hugging Face dataset to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "        analysis_results = {\n",
    "            'basic_stats': {},\n",
    "            'class_distribution': {},\n",
    "            'missing_values': {},\n",
    "            'correlations': None\n",
    "        }\n",
    "        \n",
    "        # Basic statistics\n",
    "        analysis_results['basic_stats'] = {\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'numeric_summary': df.describe().to_dict(),\n",
    "            'summary':df.info().to_dict()\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # Class distribution for categorical columns\n",
    "        for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "            analysis_results['class_distribution'][col] = dict(df[col].value_counts())\n",
    "        \n",
    "        # Missing values analysis\n",
    "        analysis_results['missing_values'] = dict(df.isnull().sum())\n",
    "        \n",
    "        # Correlation analysis for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            analysis_results['correlations'] = df[numeric_cols].corr().to_dict()\n",
    "        \n",
    "        return analysis_results\n",
    "\n",
    "    def generate_visualizations(self, df: pd.DataFrame, analysis_results: Dict) -> List[str]:\n",
    "        \"\"\"Generate and save visualizations\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        viz_files = []\n",
    "        \n",
    "        # Distribution plots for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            plt.figure(figsize=(15, 5 * ((len(numeric_cols) + 1) // 2)))\n",
    "            for i, col in enumerate(numeric_cols, 1):\n",
    "                plt.subplot((len(numeric_cols) + 1) // 2, 2, i)\n",
    "                try:\n",
    "                    sns.histplot(df[col], kde=True)\n",
    "                except Exception as e:\n",
    "                    plt.hist(df[col])\n",
    "                plt.title(f'Distribution of {col}')\n",
    "            plt.tight_layout()\n",
    "            filename = f'analysis_workspace/visualizations/numeric_distributions_{timestamp}.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            viz_files.append(filename)\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        if analysis_results['correlations']:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            correlation_df = pd.DataFrame(analysis_results['correlations'])\n",
    "            sns.heatmap(correlation_df, \n",
    "                       annot=True, \n",
    "                       cmap='coolwarm', \n",
    "                       center=0,\n",
    "                       square=True)\n",
    "            plt.title('Correlation Heatmap')\n",
    "            filename = f'analysis_workspace/visualizations/correlation_heatmap_{timestamp}.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            viz_files.append(filename)\n",
    "        \n",
    "        return viz_files\n",
    "\n",
    "    def generate_report(self, analysis_results: Dict, viz_files: List[str]) -> str:\n",
    "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f'analysis_workspace/analysis_report_{timestamp}.md'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(\"# Data Analysis Report\\n\\n\")\n",
    "            \n",
    "            # Dataset Overview\n",
    "            f.write(\"## Dataset Overview\\n\")\n",
    "            f.write(f\"- Number of rows: {analysis_results['basic_stats']['rows']}\\n\")\n",
    "            f.write(f\"- Number of columns: {analysis_results['basic_stats']['columns']}\\n\\n\")\n",
    "            \n",
    "            # Column Types\n",
    "            f.write(\"## Column Types\\n\")\n",
    "            for col, dtype in analysis_results['basic_stats']['dtypes'].items():\n",
    "                f.write(f\"- {col}: {dtype}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Missing Values\n",
    "            f.write(\"## Missing Values\\n\")\n",
    "            for col, count in analysis_results['missing_values'].items():\n",
    "                f.write(f\"- {col}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Class Distribution\n",
    "            if analysis_results['class_distribution']:\n",
    "                f.write(\"## Class Distribution\\n\")\n",
    "                for col, distribution in analysis_results['class_distribution'].items():\n",
    "                    f.write(f\"### {col}\\n\")\n",
    "                    for category, count in distribution.items():\n",
    "                        f.write(f\"- {category}: {count}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            # Visualizations\n",
    "            f.write(\"## Visualizations\\n\")\n",
    "            for viz_file in viz_files:\n",
    "                f.write(f\"![{os.path.basename(viz_file)}]({viz_file})\\n\\n\")\n",
    "        \n",
    "        return report_filename\n",
    "\n",
    "    def run_analysis(self, dataset_name: str, subset: str = None):\n",
    "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = self.load_and_process_dataset(dataset_name, subset)\n",
    "            if df is None:\n",
    "                return \"Failed to load dataset\"\n",
    "\n",
    "            # Perform analysis\n",
    "            analysis_results = self.analyze_dataset(df)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            viz_files = self.generate_visualizations(df, analysis_results)\n",
    "            \n",
    "            # Generate report\n",
    "            report_file = self.generate_report(analysis_results, viz_files)\n",
    "            \n",
    "            print(f\"Analysis complete. Report generated: {report_file}\")\n",
    "            return f\"Analysis complete. Report generated: {report_file}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {e}\")\n",
    "            return f\"Error during analysis: {e}\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the system\n",
    "    system = DataAnalysisSystem()\n",
    "    \n",
    "    # Example dataset: \"emotion\" from Hugging Face\n",
    "    result = system.run_analysis(\"emotion\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd44f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Union\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class MultiDatasetAnalysisSystem:\n",
    "    def __init__(self, datasets: Union[str, List[str]]):\n",
    "        \"\"\"\n",
    "        Initialize the system with one or multiple datasets\n",
    "        \n",
    "        Args:\n",
    "            datasets (str or List[str]): Single dataset name or list of dataset names\n",
    "        \"\"\"\n",
    "        # Ensure datasets is a list\n",
    "        self.datasets = [datasets] if isinstance(datasets, str) else datasets\n",
    "        \n",
    "        # Create workspace directory\n",
    "        self.workspace_dir = \"multi_dataset_analysis_workspace\"\n",
    "        os.makedirs(self.workspace_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{self.workspace_dir}/visualizations\", exist_ok=True)\n",
    "\n",
    "    def load_and_process_dataset(self, dataset_name: str, subset: str = None):\n",
    "        \"\"\"Load dataset from Hugging Face and perform initial processing\"\"\"\n",
    "        try:\n",
    "            print(f\"Attempting to load dataset: {dataset_name}\")\n",
    "            \n",
    "            # Try to load the dataset\n",
    "            dataset = load_dataset(dataset_name, subset) if subset else load_dataset(dataset_name)\n",
    "            \n",
    "            # Determine which split to use\n",
    "            if 'train' in dataset:\n",
    "                split = 'train'\n",
    "            elif len(dataset.keys()) > 0:\n",
    "                split = list(dataset.keys())[0]\n",
    "            else:\n",
    "                raise ValueError(\"No suitable data split found\")\n",
    "            \n",
    "            return self._convert_to_pandas(dataset[split])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _convert_to_pandas(self, dataset):\n",
    "        \"\"\"Convert Hugging Face dataset to pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "    def analyze_multiple_datasets(self):\n",
    "        \"\"\"\n",
    "        Analyze multiple datasets and generate comprehensive reports\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Analysis results for each dataset\n",
    "        \"\"\"\n",
    "        overall_results = {}\n",
    "        \n",
    "        # Loop through each dataset\n",
    "        for dataset_name in self.datasets:\n",
    "            print(f\"\\n--- Analyzing Dataset: {dataset_name} ---\")\n",
    "            \n",
    "            # Load dataset\n",
    "            df = self.load_and_process_dataset(dataset_name)\n",
    "            if df is None:\n",
    "                print(f\"Skipping {dataset_name} due to loading error\")\n",
    "                continue\n",
    "            \n",
    "            # Perform analysis\n",
    "            analysis_results = self.analyze_dataset(df)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            viz_files = self.generate_visualizations(df, analysis_results, dataset_name)\n",
    "            \n",
    "            # Generate report\n",
    "            report_file = self.generate_report(analysis_results, viz_files, dataset_name)\n",
    "            \n",
    "            # Store results\n",
    "            overall_results[dataset_name] = {\n",
    "                'analysis_results': analysis_results,\n",
    "                'report_file': report_file,\n",
    "                'visualizations': viz_files\n",
    "            }\n",
    "        \n",
    "        # Generate comparative report\n",
    "        self.generate_comparative_report(overall_results)\n",
    "        \n",
    "        return overall_results\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Perform comprehensive data analysis\"\"\"\n",
    "        analysis_results = {\n",
    "            'basic_stats': {},\n",
    "            'class_distribution': {},\n",
    "            'missing_values': {},\n",
    "            'correlations': None\n",
    "        }\n",
    "        \n",
    "        # Basic statistics\n",
    "        analysis_results['basic_stats'] = {\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'numeric_summary': df.describe().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Class distribution for categorical columns\n",
    "        for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "            analysis_results['class_distribution'][col] = dict(df[col].value_counts())\n",
    "        \n",
    "        # Missing values analysis\n",
    "        analysis_results['missing_values'] = dict(df.isnull().sum())\n",
    "        \n",
    "        # Correlation analysis for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            analysis_results['correlations'] = df[numeric_cols].corr().to_dict()\n",
    "        \n",
    "        return analysis_results\n",
    "\n",
    "    def generate_visualizations(self, df: pd.DataFrame, analysis_results: Dict, dataset_name: str) -> List[str]:\n",
    "        \"\"\"Generate and save visualizations\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        viz_files = []\n",
    "        \n",
    "        # Visualization directory for this dataset\n",
    "        dataset_viz_dir = f\"{self.workspace_dir}/visualizations/{dataset_name}\"\n",
    "        os.makedirs(dataset_viz_dir, exist_ok=True)\n",
    "        \n",
    "        # Distribution plots for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            plt.figure(figsize=(15, 5 * ((len(numeric_cols) + 1) // 2)))\n",
    "            for i, col in enumerate(numeric_cols, 1):\n",
    "                plt.subplot((len(numeric_cols) + 1) // 2, 2, i)\n",
    "                try:\n",
    "                    sns.histplot(df[col], kde=True)\n",
    "                except Exception as e:\n",
    "                    plt.hist(df[col])\n",
    "                plt.title(f'Distribution of {col}')\n",
    "            plt.tight_layout()\n",
    "            filename = f'{dataset_viz_dir}/numeric_distributions_{timestamp}.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            viz_files.append(filename)\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        if analysis_results['correlations']:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            correlation_df = pd.DataFrame(analysis_results['correlations'])\n",
    "            sns.heatmap(correlation_df, \n",
    "                       annot=True, \n",
    "                       cmap='coolwarm', \n",
    "                       center=0,\n",
    "                       square=True)\n",
    "            plt.title(f'Correlation Heatmap - {dataset_name}')\n",
    "            filename = f'{dataset_viz_dir}/correlation_heatmap_{timestamp}.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()\n",
    "            viz_files.append(filename)\n",
    "        \n",
    "        return viz_files\n",
    "\n",
    "    def generate_report(self, analysis_results: Dict, viz_files: List[str], dataset_name: str) -> str:\n",
    "        \"\"\"Generate comprehensive analysis report for a single dataset\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f'{self.workspace_dir}/{dataset_name}_analysis_report_{timestamp}.md'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(f\"# Data Analysis Report - {dataset_name}\\n\\n\")\n",
    "            \n",
    "            # Dataset Overview\n",
    "            f.write(\"## Dataset Overview\\n\")\n",
    "            f.write(f\"- Number of rows: {analysis_results['basic_stats']['rows']}\\n\")\n",
    "            f.write(f\"- Number of columns: {analysis_results['basic_stats']['columns']}\\n\\n\")\n",
    "            \n",
    "            # Column Types\n",
    "            f.write(\"## Column Types\\n\")\n",
    "            for col, dtype in analysis_results['basic_stats']['dtypes'].items():\n",
    "                f.write(f\"- {col}: {dtype}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Missing Values\n",
    "            f.write(\"## Missing Values\\n\")\n",
    "            for col, count in analysis_results['missing_values'].items():\n",
    "                f.write(f\"- {col}: {count}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Visualizations\n",
    "            f.write(\"## Visualizations\\n\")\n",
    "            for viz_file in viz_files:\n",
    "                f.write(f\"![{os.path.basename(viz_file)}]({viz_file})\\n\\n\")\n",
    "        \n",
    "        return report_filename\n",
    "\n",
    "    def generate_comparative_report(self, overall_results: Dict):\n",
    "        \"\"\"Generate a comparative report across all datasets\"\"\"\n",
    "        comparative_report_path = f'{self.workspace_dir}/comparative_analysis_report.md'\n",
    "        \n",
    "        with open(comparative_report_path, 'w') as f:\n",
    "            f.write(\"# Comparative Dataset Analysis Report\\n\\n\")\n",
    "            \n",
    "            # Comparative Statistics\n",
    "            f.write(\"## Comparative Dataset Statistics\\n\")\n",
    "            for dataset_name, results in overall_results.items():\n",
    "                analysis = results['analysis_results']\n",
    "                f.write(f\"### {dataset_name}\\n\")\n",
    "                f.write(f\"- Rows: {analysis['basic_stats']['rows']}\\n\")\n",
    "                f.write(f\"- Columns: {analysis['basic_stats']['columns']}\\n\")\n",
    "                f.write(f\"- Missing Values: {sum(analysis['missing_values'].values())}\\n\\n\")\n",
    "            \n",
    "            # Visualization Comparison Section\n",
    "            f.write(\"## Visualizations Comparison\\n\")\n",
    "            for dataset_name, results in overall_results.items():\n",
    "                f.write(f\"### {dataset_name} Visualizations\\n\")\n",
    "                for viz_file in results['visualizations']:\n",
    "                    f.write(f\"![{os.path.basename(viz_file)}]({viz_file})\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac462ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Dataset: emotion ---\n",
      "Attempting to load dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Dataset: imdb ---\n",
      "Attempting to load dataset: imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: emotion\n",
      "Report File: multi_dataset_analysis_workspace/emotion_analysis_report_20241217_185445.md\n",
      "Visualizations: ['multi_dataset_analysis_workspace/visualizations/emotion/numeric_distributions_20241217_185445.png']\n",
      "\n",
      "Dataset: imdb\n",
      "Report File: multi_dataset_analysis_workspace/imdb_analysis_report_20241217_185458.md\n",
      "Visualizations: ['multi_dataset_analysis_workspace/visualizations/imdb/numeric_distributions_20241217_185457.png']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List of datasets to analyze\n",
    "    datasets_to_analyze = [\n",
    "        \"emotion\",      # Text emotion dataset\n",
    "        \"imdb\",         # Movie review sentiment dataset\n",
    "    ]\n",
    "\n",
    "    # Initialize multi-dataset analysis system\n",
    "    multi_analysis_system = MultiDatasetAnalysisSystem(datasets_to_analyze)\n",
    "    \n",
    "    # Run analysis on multiple datasets\n",
    "    results = multi_analysis_system.analyze_multiple_datasets()\n",
    "    \n",
    "    # Print summary of analysis\n",
    "    for dataset, details in results.items():\n",
    "        print(f\"\\nDataset: {dataset}\")\n",
    "        print(f\"Report File: {details['report_file']}\")\n",
    "        print(f\"Visualizations: {details['visualizations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfd948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
