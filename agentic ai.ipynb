{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36087110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing dataset: imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\AppData\\Local\\Temp\\ipykernel_48628\\3464753542.py:44: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: str(x) if isinstance(x, dict) else x)\n",
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing dataset: emotion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhi\\AppData\\Local\\Temp\\ipykernel_48628\\3464753542.py:44: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: str(x) if isinstance(x, dict) else x)\n",
      "C:\\Users\\Nidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "class ComprehensiveDatasetAnalyzer:\n",
    "    def __init__(self, datasets, model_name=\"distilbert-base-uncased-distilled-squad\"):\n",
    "        \n",
    "        self.datasets = datasets\n",
    "        self.workspace_dir = \"comprehensive_dataset_analysis_workspace\"\n",
    "        os.makedirs(self.workspace_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{self.workspace_dir}/reports\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.workspace_dir}/visualizations\", exist_ok=True)\n",
    "\n",
    "        # Load QA model for text insights\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def perform_comprehensive_analysis(self):\n",
    "\n",
    "        comprehensive_results = {}\n",
    "\n",
    "        for dataset_name in self.datasets:\n",
    "            try:\n",
    "                print(f\"\\nAnalyzing dataset: {dataset_name}\")\n",
    "                dataset = load_dataset(dataset_name)\n",
    "\n",
    "                # split by keys\n",
    "                split = list(dataset.keys())[0]\n",
    "                data = dataset[split]\n",
    "\n",
    "                # Converting HuggingFace dataset to a Pandas DataFrame\n",
    "                try:\n",
    "                    df = pd.DataFrame(data)\n",
    "                except Exception as e:\n",
    "                    df = pd.DataFrame(data.to_pandas())\n",
    "                \n",
    "                # separate key value by using applymap\n",
    "                df = df.applymap(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "                # Perform analysis\n",
    "                analysis_results = {\n",
    "                    \"exploratory_analysis\": self._perform_exploratory_data_analysis(df),\n",
    "                    \"preprocessing_needs\": self._analyze_preprocessing_needs(df),\n",
    "                    \"text_insights\": self._generate_text_insights(df),\n",
    "                }\n",
    "\n",
    "                # Generate visualizations\n",
    "                analysis_results[\"visualizations\"] = self._generate_visualizations(df, dataset_name)\n",
    "\n",
    "                # Generate reports\n",
    "                self._generate_comprehensive_report(analysis_results, dataset_name)\n",
    "\n",
    "                comprehensive_results[dataset_name] = analysis_results\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {dataset_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "        # Generate a comparative report\n",
    "        self._generate_comparative_report(comprehensive_results)\n",
    "\n",
    "        return comprehensive_results\n",
    "\n",
    "    def _perform_exploratory_data_analysis(self, df):\n",
    "        eda_results = {\n",
    "            \"basic_info\": {\n",
    "                \"total_rows\": len(df),\n",
    "                \"total_columns\": len(df.columns),\n",
    "                \"column_types\": dict(df.dtypes),\n",
    "            },\n",
    "            \"summary_statistics\": {},\n",
    "            \"column_details\": {},\n",
    "        }\n",
    "\n",
    "        # Numeric column analysis\n",
    "        numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            eda_results[\"summary_statistics\"] = df[numeric_cols].describe().to_dict()\n",
    "\n",
    "        #  column analysis for prepreocessing\n",
    "        for column in df.columns:\n",
    "            col_details = {\n",
    "                \"unique_values\": df[column].nunique(),\n",
    "                \"null_count\": df[column].isnull().sum(),\n",
    "                \"null_percentage\": (df[column].isnull().sum() / len(df)) * 100,\n",
    "            }\n",
    "            eda_results[\"column_details\"][column] = col_details\n",
    "\n",
    "        return eda_results\n",
    "\n",
    "    def _analyze_preprocessing_needs(self, df):\n",
    "        \"\"\"Analyze preprocessing requirements.\"\"\"\n",
    "        return {\n",
    "            \"missing_values\": self._detect_missing_values(df),\n",
    "            \"duplicate_rows\": self._detect_duplicate_rows(df),\n",
    "        }\n",
    "\n",
    "    def _detect_missing_values(self, df):\n",
    "        \"\"\"Detect and analyze missing values.\"\"\"\n",
    "        return df.isnull().sum().to_dict()\n",
    "\n",
    "    def _detect_duplicate_rows(self, df):\n",
    "        \"\"\"Detect duplicate rows.\"\"\"\n",
    "        return {\"total_duplicates\": df.duplicated().sum()}\n",
    "\n",
    "    def _generate_text_insights(self, df):\n",
    "        \"\"\"Generate text insights using a QA model.\"\"\"\n",
    "        text_insights = {}\n",
    "        text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "        for col in text_cols:\n",
    "            try:\n",
    "                text_sample = df[col].dropna().sample(n=1, random_state=1).iloc[0]\n",
    "                if len(str(text_sample)) > 20:  # Ensure valid context\n",
    "                    insights = self.qa_pipeline(\n",
    "                        {\"question\": \"What is the main idea?\", \"context\": text_sample}\n",
    "                    )\n",
    "                    text_insights[col] = insights[\"answer\"]\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        return text_insights\n",
    "\n",
    "    def _generate_visualizations(self, df, dataset_name):\n",
    "        \"\"\"Generate visualizations for numeric columns.\"\"\"\n",
    "        viz_files = []\n",
    "        viz_dir = f\"{self.workspace_dir}/visualizations/{dataset_name}\"\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(df[col], kde=True)\n",
    "            plt.title(f\"Distribution of {col}\")\n",
    "            plot_path = f\"{viz_dir}/{col}_distribution.png\"\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            viz_files.append(plot_path)\n",
    "\n",
    "        return viz_files\n",
    "\n",
    "    def _generate_comprehensive_report(self, analysis_results, dataset_name):\n",
    "        \"\"\"Generate a comprehensive report for a single dataset.\"\"\"\n",
    "        report_file = f\"{self.workspace_dir}/reports/{dataset_name}_report.txt\"\n",
    "        with open(report_file, \"w\") as f:\n",
    "            for key, value in analysis_results.items():\n",
    "                f.write(f\"{key.upper()}:\\n\")\n",
    "                f.write(f\"{value}\\n\\n\")\n",
    "\n",
    "    def _generate_comparative_report(self, comprehensive_results):\n",
    "        \"\"\"Generate a comparative report across datasets.\"\"\"\n",
    "        comparative_file = f\"{self.workspace_dir}/reports/comparative_report.txt\"\n",
    "        with open(comparative_file, \"w\") as f:\n",
    "            for dataset_name, results in comprehensive_results.items():\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"{results}\\n\\n\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    datasets_to_analyze = [\"imdb\", \"emotion\",'/phihung/titanic']\n",
    "\n",
    "    analyzer = ComprehensiveDatasetAnalyzer(datasets_to_analyze)\n",
    "    results = analyzer.perform_comprehensive_analysis()\n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c14ad2fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fpdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# For NLP insights\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPDF  \u001b[38;5;66;03m# For generating PDF reports\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fpdf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import pipeline  # For NLP insights\n",
    "from fpdf import FPDF  # For generating PDF reports\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class ComprehensiveDatasetAnalyzer:\n",
    "    def __init__(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def analyze_nlp_data(self, dataset_name, data):\n",
    "        \"\"\"Analyze text-based NLP datasets.\"\"\"\n",
    "        print(f\"Analyzing NLP dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        # Basic text statistics\n",
    "        data['text_length'] = data['text'].apply(len)\n",
    "        data['word_count'] = data['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        results['text_length_stats'] = data['text_length'].describe()\n",
    "        results['word_count_stats'] = data['word_count'].describe()\n",
    "\n",
    "        # NLP insights using a small LLM\n",
    "        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\")\n",
    "        data['sentiment'] = data['text'].apply(lambda x: sentiment_analyzer(x)[0]['label'])\n",
    "        results['sentiment_distribution'] = data['sentiment'].value_counts()\n",
    "\n",
    "        self.generate_visualizations(\"nlp\", dataset_name, data, results)\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"nlp\")\n",
    "        return results\n",
    "\n",
    "    def analyze_image_data(self, dataset_name, image_paths):\n",
    "        \"\"\"Analyze image datasets.\"\"\"\n",
    "        print(f\"Analyzing Image dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        brightness_values = []\n",
    "        pixel_values = []\n",
    "\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Grayscale image\n",
    "                pixel_values.append(img.flatten())\n",
    "                brightness_values.append(np.mean(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "\n",
    "        results['brightness_stats'] = pd.Series(brightness_values).describe()\n",
    "\n",
    "        # Visualize pixel intensity distribution\n",
    "        self.generate_visualizations(\"image\", dataset_name, image_paths, results)\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"image\")\n",
    "        return results\n",
    "\n",
    "    def analyze_structured_data(self, dataset_name, data):\n",
    "        \"\"\"Analyze structured tabular data.\"\"\"\n",
    "        print(f\"Analyzing Structured dataset: {dataset_name}\")\n",
    "        results = {}\n",
    "\n",
    "        results['info'] = str(data.info())\n",
    "        results['description'] = data.describe().to_string()\n",
    "        results['null_values'] = data.isnull().sum()\n",
    "        results['duplicates'] = data[data.duplicated()].index.tolist()\n",
    "        results['outliers'] = {col: self.detect_outliers(data[col]) for col in data.select_dtypes(include=np.number)}\n",
    "\n",
    "        self.generate_visualizations(\"structured\", dataset_name, data, results)\n",
    "        self.export_pdf_report(dataset_name, results, data_type=\"structured\")\n",
    "        return results\n",
    "\n",
    "    def detect_outliers(self, series):\n",
    "        \"\"\"Detect outliers in a numerical series using the IQR method.\"\"\"\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = series[(series < (Q1 - 1.5 * IQR)) | (series > (Q3 + 1.5 * IQR))]\n",
    "        return outliers.index.tolist()\n",
    "\n",
    "    def generate_visualizations(self, data_type, dataset_name, data, results):\n",
    "        \"\"\"Generate and save visualizations specific to the data type.\"\"\"\n",
    "        output_dir = f\"visualizations/{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if data_type == \"nlp\":\n",
    "            plt.figure()\n",
    "            sns.histplot(data['text_length'], kde=True)\n",
    "            plt.title(\"Text Length Distribution\")\n",
    "            plt.savefig(f\"{output_dir}/text_length_distribution.png\")\n",
    "\n",
    "        elif data_type == \"image\":\n",
    "            plt.figure()\n",
    "            sns.histplot(results['brightness_stats'], kde=True)\n",
    "            plt.title(\"Image Brightness Distribution\")\n",
    "            plt.savefig(f\"{output_dir}/brightness_distribution.png\")\n",
    "\n",
    "        elif data_type == \"structured\":\n",
    "            plt.figure()\n",
    "            sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "            plt.title(\"Correlation Matrix\")\n",
    "            plt.savefig(f\"{output_dir}/correlation_matrix.png\")\n",
    "\n",
    "    def export_pdf_report(self, dataset_name, analysis_results, data_type=\"general\"):\n",
    "        \"\"\"Export a consolidated analysis report as a PDF.\"\"\"\n",
    "        pdf = FPDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "        pdf.cell(200, 10, txt=f\"Analysis Report for {dataset_name}\", ln=True, align=\"C\")\n",
    "        pdf.ln(10)\n",
    "\n",
    "        for key, value in analysis_results.items():\n",
    "            pdf.set_font(\"Arial\", style=\"B\", size=12)\n",
    "            pdf.cell(200, 10, txt=str(key), ln=True)\n",
    "            pdf.set_font(\"Arial\", size=10)\n",
    "            pdf.multi_cell(0, 10, txt=str(value))\n",
    "            pdf.ln(5)\n",
    "\n",
    "        output_path = f\"reports/{dataset_name}_analysis_report.pdf\"\n",
    "        os.makedirs(\"reports\", exist_ok=True)\n",
    "        pdf.output(output_path)\n",
    "        print(f\"PDF report saved at {output_path}\")\n",
    "\n",
    "    def perform_comprehensive_analysis(self):\n",
    "        \"\"\"Analyze all datasets and direct them to their relevant type function.\"\"\"\n",
    "        comprehensive_results = {}\n",
    "\n",
    "        for dataset_name in self.datasets:\n",
    "            print(f\"Analyzing dataset: {dataset_name}\")\n",
    "\n",
    "            try:\n",
    "                # Identify dataset type and analyze accordingly\n",
    "                if dataset_name == \"imdb\":  # NLP Example\n",
    "                    data = pd.DataFrame({\"text\": [\"This is a sample review.\", \"Another review.\"]})\n",
    "                    comprehensive_results[dataset_name] = self.analyze_nlp_data(dataset_name, data)\n",
    "\n",
    "                elif dataset_name == \"image-dataset\":  # Image Example\n",
    "                    image_paths = [\"path_to_image1.jpg\", \"path_to_image2.jpg\"]\n",
    "                    comprehensive_results[dataset_name] = self.analyze_image_data(dataset_name, image_paths)\n",
    "\n",
    "                elif dataset_name == \"titanic\":  # Structured Example\n",
    "                    data = sns.load_dataset(\"titanic\")\n",
    "                    comprehensive_results[dataset_name] = self.analyze_structured_data(dataset_name, data)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Unknown dataset type for {dataset_name}, skipping.\")\n",
    "\n",
    "            except Exception:\n",
    "                print(f\"Error analyzing {dataset_name}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        return comprehensive_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    datasets_to_analyze = [\"imdb\", \"titanic\", \"image-dataset\"]\n",
    "    analyzer = ComprehensiveDatasetAnalyzer(datasets_to_analyze)\n",
    "    results = analyzer.perform_comprehensive_analysis()\n",
    "    print(\"Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
